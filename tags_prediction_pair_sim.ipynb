{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read train file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all_train: {image file name - image tag}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_root_path = \"data/0813/DatasetA_train_20180813\"\n",
    "test_root_path = \"data/0813/DatasetA_test_20180813/DatasetA_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_train = pd.read_csv(os.path.join(train_root_path, 'train.txt'), sep='\\t', header=None)\n",
    "all_train.columns = ['filename', 'tag']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to image file name in all_train, get image in order  \n",
    "X_train: {image in array}, in shape (image_count, height, width, channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-divided Train and Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set has 30170 color images\n",
      "Validation set has 8051 color images.\n",
      "Image shape: (100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "Y_train = pd.read_csv(os.path.join(train_root_path, 'train_local.txt'), sep='\\t', header=None)\n",
    "Y_train.columns = ['filename', 'tag']\n",
    "\n",
    "Y_valid = pd.read_csv(os.path.join(train_root_path, 'dev_local.txt'), sep='\\t', header=None)\n",
    "Y_valid.columns = ['filename', 'tag']\n",
    "\n",
    "rootpath = os.path.join(train_root_path, \"train\") #文件夹目录\n",
    "X_train = []\n",
    "X_valid = []\n",
    "\n",
    "for filename in Y_train['filename']: #遍历文件夹\n",
    "    pic = image.load_img(rootpath + \"/\" + filename, target_size = (100, 100))\n",
    "    pic = image.img_to_array(pic)\n",
    "    X_train.append(pic)\n",
    "    \n",
    "for filename in Y_valid['filename']: #遍历文件夹\n",
    "    pic = image.load_img(rootpath + \"/\" + filename, target_size = (100, 100))\n",
    "    pic = image.img_to_array(pic)\n",
    "    X_valid.append(pic)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_valid = np.array(X_valid)\n",
    "print('Train set has {} color images'.format(X_train.shape[0]))\n",
    "print('Validation set has {} color images.'.format(X_valid.shape[0]))\n",
    "\n",
    "print('Image shape:', X_train[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_files = pd.read_csv(os.path.join(test_root_path, 'image.txt'), sep='\\t', header=None)\n",
    "test_files.columns = ['filename']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set has 14633 color images\n",
      "Image shape: (100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "rootpath = os.path.join(test_root_path, \"test\") #文件夹目录\n",
    "\n",
    "X_test = []\n",
    "\n",
    "for filename in test_files['filename']: #遍历文件夹\n",
    "    pic = image.load_img(rootpath + \"/\" + filename, target_size = (100, 100))\n",
    "    pic = image.img_to_array(pic)\n",
    "    X_test.append(pic)\n",
    "    \n",
    "X_test = np.array(X_test)\n",
    "print('Test set has {} color images'.format(X_test.shape[0]))\n",
    "print('Image shape:',X_test[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tag2attr: {image tag - attribute name} pd form, merged from attr2name {attribute name}  \n",
    "tag2label: {image tag - tag name} match image tag to English name  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag2attr = pd.read_csv(os.path.join(train_root_path, 'attributes_per_class.txt'),sep='\\t', header=None)\n",
    "attr2name = pd.read_csv(os.path.join(train_root_path, 'attribute_list.txt'), sep='\\t', header=None)\n",
    "tag2label = pd.read_csv(os.path.join(train_root_path, 'label_list.txt'), sep='\\t', header=None)\n",
    "tag2label.columns = ['tag', 'label']\n",
    "attr2name = attr2name.drop([0], axis=1)\n",
    "attr2name.columns = ['name']\n",
    "col_name = ['tag'] + attr2name['name'].tolist()\n",
    "tag2attr.columns = col_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the provided word vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "embed_path = os.path.join(train_root_path, 'class_wordembeddings.txt')\n",
    "with open(embed_path,'r') as f:\n",
    "    for line in f:\n",
    "        word, vec = line.split(' ', 1)\n",
    "        word_dict[word] = np.array(list(map(float, vec.split())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Zero-shot tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_tags = all_train['tag'].unique()\n",
    "total_tags = tag2attr['tag'].unique()\n",
    "zero_shot_tags = list(set(total_tags) - set(train_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230 unique tags in total (Including Test Data). \n",
      "190 unique tags in Training Data. \n",
      "40 Zero-shot tags.\n"
     ]
    }
   ],
   "source": [
    "print(\"{} unique tags in total (Including Test Data). \\n{} unique tags in Training Data. \\n{} Zero-shot tags.\".format(\n",
    "    len(total_tags), len(train_tags), len(zero_shot_tags)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all_train_attr: {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def img_label_attr(df, tag2label=tag2label, tag2attr=tag2attr):\n",
    "    Y_cate = []\n",
    "    for tag in df['tag']:\n",
    "        arr = [tag2label[tag2label['tag'] == tag].iloc[0,:].tolist()[1]] + tag2attr[tag2attr['tag'] == tag].iloc[0,:].tolist()\n",
    "        Y_cate.append(arr)\n",
    "        \n",
    "    train_attr = pd.DataFrame(Y_cate)\n",
    "    col_name = ['label'] + ['tag'] + attr2name['name'].tolist()\n",
    "    train_attr.columns = col_name\n",
    "    print('Converted {} image tags into categories'.format(train_attr.shape[0]))\n",
    "    return train_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# 这个很慢\n",
    "def build_pair_dataset(X, Y, tag2label=tag2label, tag2attr=tag2attr):\n",
    "    Y_cate = []\n",
    "    label_match = []\n",
    "    X_new = []\n",
    "    NEG_NUM = 8\n",
    "    tags = list(tag2label['tag'].values)\n",
    "    for tag, data in zip(Y['tag'], X):\n",
    "        arr = [tag2label[tag2label['tag'] == tag].iloc[0,:].tolist()[1]] + tag2attr[tag2attr['tag'] == tag].iloc[0,:].tolist()\n",
    "        Y_cate.append(arr)\n",
    "        label_match.append(1)\n",
    "        sample_neg_tags = []\n",
    "        X_new.append(data)\n",
    "        while True:\n",
    "            tmp = random.sample(tags, 1)[0]\n",
    "            if tmp != tag and tmp not in sample_neg_tags:\n",
    "                sample_neg_tags.append(tmp)\n",
    "            if len(sample_neg_tags) >= NEG_NUM:\n",
    "                break\n",
    "        for neg_tag in sample_neg_tags:\n",
    "            arr = [tag2label[tag2label['tag'] == neg_tag].iloc[0,:].tolist()[1]] + tag2attr[tag2attr['tag'] == neg_tag].iloc[0,:].tolist()\n",
    "            Y_cate.append(arr)\n",
    "            label_match.append(0)\n",
    "            X_new.append(data)\n",
    "        \n",
    "    Y_attr = pd.DataFrame(Y_cate)\n",
    "    col_name = ['label'] + ['tag'] + attr2name['name'].tolist()\n",
    "    Y_attr.columns = col_name\n",
    "    print('Converted {} image tags into categories'.format(Y_attr.shape[0]))\n",
    "    return X_new, Y_attr, label_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 271530 image tags into categories\n"
     ]
    }
   ],
   "source": [
    "X_new_train, Y_attr_train, label_match_train = build_pair_dataset(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 72459 image tags into categories\n"
     ]
    }
   ],
   "source": [
    "X_new_valid, Y_attr_valid, label_match_valid = build_pair_dataset(X_valid, Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 30170 image tags into categories\n",
      "Converted 8051 image tags into categories\n"
     ]
    }
   ],
   "source": [
    "# Y_train_attr = img_label_attr(Y_train)\n",
    "# Y_valid_attr = img_label_attr(Y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_test = X_valid[0:10]\n",
    "Y_test = img2sematic.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cos_dis(v1, v2):\n",
    "    return np.linalg.norm(v1-v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keys = []\n",
    "values = []\n",
    "\n",
    "for key, value in word_dict.items():\n",
    "    keys.append(key)\n",
    "    values.append(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "res = []\n",
    "\n",
    "for v1 in Y_test:\n",
    "    score = []\n",
    "    for v2 in values:\n",
    "        score.append(cos_dis(v1,v2))\n",
    "    \n",
    "    label = keys[score.index(min(score))]\n",
    "    res.append(tag2label[tag2label['label'] == label]['tag'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = pd.concat([test_files,pd.DataFrame(res)],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to submit.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result.to_csv('submit.txt', sep='\\t', header=None, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch (Not Done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ZJL_DS(Dataset):\n",
    "    def __init__(self, X, Y_attr, label_match):\n",
    "        # np array\n",
    "        self.img = X\n",
    "        # series\n",
    "        self.tag = Y_attr['tag']\n",
    "        self.label = Y_attr['label']\n",
    "        # pd dataframe\n",
    "        self.attr = Y_attr.drop(['label', 'tag'], axis=1)\n",
    "        self.label_match = label_match\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_arr = self.img[idx]\n",
    "        tag = self.tag[idx]\n",
    "        label = self.label[idx]\n",
    "        attr = self.attr.iloc[idx,:]\n",
    "        label_match = self.label_match[idx]\n",
    "        info = (tag, label, attr)\n",
    "        return img_arr, info, label_match\n",
    "    \n",
    "    def show_data(self, idx):\n",
    "        img_arr = self.img[idx]\n",
    "        plt.imshow(image.array_to_img(X_train[idx]))\n",
    "        label = self.label[idx]\n",
    "        tag = self.tag[idx]\n",
    "        attr = self.attr.iloc[idx,:]\n",
    "        label_match = self.label_match[idx]\n",
    "        print(\"Image index:{} \\n\\nLabel: {} \\n\\nTag: {} \\n\\nAttribute:\\n{} \\nlabel_match: {}\"\\\n",
    "              .format(idx, label, tag, attr, label_match))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image index:2502 \n",
      "\n",
      "Label: tarantula \n",
      "\n",
      "Tag: ZJL10 \n",
      "\n",
      "Attribute:\n",
      "is animal               1.0\n",
      "is transportation       0.0\n",
      "is clothes              0.0\n",
      "is plant                0.0\n",
      "is tableware            0.0\n",
      "is device               0.0\n",
      "is black                0.5\n",
      "is white                0.0\n",
      "is blue                 0.0\n",
      "is brown                0.7\n",
      "is orange               0.0\n",
      "is red                  0.0\n",
      "is green                0.0\n",
      "is yellow               0.2\n",
      "has feathers            0.0\n",
      "has four legs           0.0\n",
      "has two legs            0.0\n",
      "has two arms            0.0\n",
      "is for entertainment    0.0\n",
      "is for business         0.0\n",
      "is for communication    0.0\n",
      "is for family           0.0\n",
      "is for office use       0.0\n",
      "is for personal         0.0\n",
      "is gorgeous             0.0\n",
      "is simple               0.0\n",
      "is elegant              0.0\n",
      "is cute                 0.0\n",
      "is pure                 0.0\n",
      "is naive                0.0\n",
      "Name: 2502, dtype: float64 \n",
      "label_match: 1\n"
     ]
    }
   ],
   "source": [
    "train_ds = ZJL_DS(X_new_train, Y_attr_train, label_match_train)\n",
    "valid_ds = ZJL_DS(X_new_valid, Y_attr_valid, label_match_valid)\n",
    "# # get train data\n",
    "# img_arr, (tag, label, attr) = train_ds[32]\n",
    "# label_embedding = word_dict[label]\n",
    "# inspect data \n",
    "train_ds.show_data(2502)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30170"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "\n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "modules=list(resnet18.children())[:-1]\n",
    "resnet18=nn.Sequential(*modules) #.cuda()\n",
    "for p in resnet18.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "class SimpleNN(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.input_linear = torch.nn.Linear(D_in, H)\n",
    "        self.middle_linear = torch.nn.Linear(H, H)\n",
    "        self.output_linear = torch.nn.Linear(H, D_out)\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        h_relu = self.input_linear(x).clamp(min=0)\n",
    "        h_relu = self.middle_linear(h_relu).clamp(min=0)\n",
    "        y_pred = self.output_linear(h_relu)\n",
    "#         y_pred = self.softmax(output)\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "class SimpleNN_pair(torch.nn.Module):\n",
    "    def __init__(self, D_in_x, D_in_y, H, dropout=0.5):\n",
    "\n",
    "        super(SimpleNN_pair, self).__init__()\n",
    "        self.input_linear_x = torch.nn.Linear(D_in_x, H)\n",
    "        self.input_linear_y = torch.nn.Linear(D_in_y, H)\n",
    "        self.middle_linear = torch.nn.Linear(H, H)\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "        \n",
    "        self.final_layers = nn.Sequential(\n",
    "            nn.Linear(2 * H, H),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(H, 2),\n",
    "            nn.LogSoftmax(1)\n",
    "        )\n",
    "        \n",
    "    def _algo_1_horiz_comp(self, sent1_block_a, sent2_block_a):\n",
    "        comparison_feats = []\n",
    "        for pool in ('max', 'min', 'mean'):\n",
    "            regM1, regM2 = [], []\n",
    "            for ws in self.filter_widths:\n",
    "                x1 = sent1_block_a[ws][pool].unsqueeze(2)\n",
    "                x2 = sent2_block_a[ws][pool].unsqueeze(2)\n",
    "                if np.isinf(ws):\n",
    "                    x1 = x1.expand(-1, self.n_holistic_filters, -1)\n",
    "                    x2 = x2.expand(-1, self.n_holistic_filters, -1)\n",
    "                regM1.append(x1)\n",
    "                regM2.append(x2)\n",
    "\n",
    "            regM1 = torch.cat(regM1, dim=2)\n",
    "            regM2 = torch.cat(regM2, dim=2)\n",
    "\n",
    "            # Cosine similarity\n",
    "            comparison_feats.append(F.cosine_similarity(regM1, regM2, dim=2))\n",
    "            # Euclidean distance\n",
    "            pairwise_distances = []\n",
    "            for x1, x2 in zip(regM1, regM2):\n",
    "                dist = F.pairwise_distance(x1, x2).view(1, -1)\n",
    "                pairwise_distances.append(dist)\n",
    "            comparison_feats.append(torch.cat(pairwise_distances))\n",
    "\n",
    "        return torch.cat(comparison_feats, dim=1)\n",
    "    \n",
    "    def _algo_2_vert_comp(self, sent1_block_a, sent2_block_a, sent1_block_b, sent2_block_b):\n",
    "        comparison_feats = []\n",
    "        ws_no_inf = [w for w in self.filter_widths if not np.isinf(w)]\n",
    "        for pool in ('max', 'min', 'mean'):\n",
    "            for ws1 in self.filter_widths:\n",
    "                x1 = sent1_block_a[ws1][pool]\n",
    "                for ws2 in self.filter_widths:\n",
    "                    x2 = sent2_block_a[ws2][pool]\n",
    "                    if (not np.isinf(ws1) and not np.isinf(ws2)) or (np.isinf(ws1) and np.isinf(ws2)):\n",
    "                        comparison_feats.append(F.cosine_similarity(x1, x2).unsqueeze(1))\n",
    "                        comparison_feats.append(F.pairwise_distance(x1, x2).unsqueeze(1))\n",
    "                        comparison_feats.append(torch.abs(x1 - x2))\n",
    "\n",
    "        for pool in ('max', 'min'):\n",
    "            for ws in ws_no_inf:\n",
    "                oG_1B = sent1_block_b[ws][pool]\n",
    "                oG_2B = sent2_block_b[ws][pool]\n",
    "                for i in range(0, self.n_per_dim_filters):\n",
    "                    x1 = oG_1B[:, :, i]\n",
    "                    x2 = oG_2B[:, :, i]\n",
    "                    comparison_feats.append(F.cosine_similarity(x1, x2).unsqueeze(1))\n",
    "                    comparison_feats.append(F.pairwise_distance(x1, x2).unsqueeze(1))\n",
    "                    comparison_feats.append(torch.abs(x1 - x2))\n",
    "\n",
    "        return torch.cat(comparison_feats, dim=1)\n",
    "    \n",
    "    def concat_attention(self, sent1, sent2):\n",
    "        sent1_transposed = sent1.transpose(1, 2)\n",
    "        attention_dot = torch.bmm(sent1_transposed, sent2)\n",
    "        sent1_norms = torch.norm(sent1_transposed, p=2, dim=2, keepdim=True)\n",
    "        sent2_norms = torch.norm(sent2, p=2, dim=1, keepdim=True)\n",
    "        attention_norms = torch.bmm(sent1_norms, sent2_norms)\n",
    "        attention_matrix = attention_dot / attention_norms\n",
    "\n",
    "        sum_row = attention_matrix.sum(2)\n",
    "        sum_col = attention_matrix.sum(1)\n",
    "\n",
    "        attention_weight_vec1 = F.softmax(sum_row, 1)\n",
    "        attention_weight_vec2 = F.softmax(sum_col, 1)\n",
    "\n",
    "        attention_weighted_sent1 = attention_weight_vec1.unsqueeze(1).expand(-1, self.n_word_dim, -1) * sent1\n",
    "        attention_weighted_sent2 = attention_weight_vec2.unsqueeze(1).expand(-1, self.n_word_dim, -1) * sent2\n",
    "        attention_emb1 = torch.cat((attention_weighted_sent1, sent1), dim=1)\n",
    "        attention_emb2 = torch.cat((attention_weighted_sent2, sent2), dim=1)\n",
    "        return attention_emb1, attention_emb2\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        y = y.float()\n",
    "        hx_relu = self.input_linear_x(x).clamp(min=0)\n",
    "        hy_relu = self.input_linear_y(y).clamp(min=0)\n",
    "        # hx_relu = self.middle_linear(hx_relu).clamp(min=0)\n",
    "        # hy_relu = self.middle_linear(hy_relu).clamp(min=0)\n",
    "        feat_all = torch.cat([hx_relu, hy_relu], dim=1)\n",
    "        y_pred = self.final_layers(feat_all)\n",
    "        return y_pred\n",
    "    \n",
    "class ImageLoader(Dataset):\n",
    "    def __init__(self, data, transform):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def build_attr(self, attr):\n",
    "        res = []\n",
    "        for i in attr:\n",
    "            res.append(i)\n",
    "        return np.array(res)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform(np.uint8(self.data[idx][0])), \\\n",
    "                self.build_attr(self.data[idx][1][2]), \\\n",
    "                self.data[idx][2]\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer):\n",
    "    losses = AverageMeter()\n",
    "    model.train()\n",
    "    \n",
    "    for i_batch, (sample_batched, sample_attr, label_match) in enumerate(train_loader):\n",
    "        output = resnet18(Variable(sample_batched)).squeeze() # .cuda()\n",
    "\n",
    "        y_pred = model(output, sample_attr)\n",
    "\n",
    "        loss = criterion(y_pred, label_match) # sample_attr.type(torch.cuda.FloatTensor)\n",
    "        \n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss = loss.float()\n",
    "        losses.update(loss.data[0], sample_batched.size(0))\n",
    "        if i_batch % 1000 == 0:\n",
    "            print(\"batch {}, loss: {}\".format(i_batch, losses.val.cpu().numpy()))\n",
    "    return losses.avg.cpu().numpy()\n",
    "\n",
    "def validation(val_loader, model, criterion):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    \n",
    "    for i_batch, (sample_batched, sample_attr) in enumerate(val_loader):\n",
    "        output = resnet18(Variable(sample_batched).cuda()).squeeze()\n",
    "\n",
    "        y_pred = model(output)\n",
    "\n",
    "        loss = criterion(y_pred, sample_attr.type(torch.cuda.FloatTensor))\n",
    "        \n",
    "        loss = loss.float()\n",
    "        losses.update(loss.data[0], sample_batched.size(0))\n",
    "        \n",
    "    return losses.avg.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victor/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:170: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6481333\n",
      "0.5940957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-17:\n",
      "Process Process-20:\n",
      "Process Process-18:\n",
      "Process Process-19:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/victor/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/victor/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/victor/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/victor/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/victor/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/victor/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/victor/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/victor/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/Users/victor/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/Users/victor/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/Users/victor/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/Users/victor/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/Users/victor/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/Users/victor/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/Users/victor/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/Users/victor/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/Users/victor/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/Users/victor/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/Users/victor/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/Users/victor/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/Users/victor/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/Users/victor/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/Users/victor/anaconda3/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/Users/victor/anaconda3/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/Users/victor/anaconda3/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/victor/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/victor/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/Users/victor/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/Users/victor/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/Users/victor/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/victor/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/Users/victor/anaconda3/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x19a405d0b8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/victor/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 397, in __del__\n",
      "    def __del__(self):\n",
      "  File \"/Users/victor/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 227, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 20736) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-93a0518e8d00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-6a8608e6e3f1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, criterion, optimizer)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msample_batched\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_attr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_match\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet18\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_batched\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# .cuda()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 301\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "input_mean = [0.485, 0.456, 0.406]\n",
    "input_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "t_transforms = [\n",
    "    torchvision.transforms.ToPILImage(),\n",
    "    torchvision.transforms.Resize((224,224)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(input_mean, input_std),\n",
    "]\n",
    "transformer = torchvision.transforms.Compose(t_transforms)\n",
    "\n",
    "\n",
    "train_dataset = ImageLoader(train_ds, transformer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4,\n",
    "                        shuffle=True, num_workers=4)\n",
    "\n",
    "valid_dataset = ImageLoader(valid_ds, transformer)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=4,\n",
    "                        shuffle=True, num_workers=4)\n",
    "\n",
    "model = SimpleNN_pair(512, 30, 128) # .cuda()\n",
    "\n",
    "# criterion = torch.nn.MSELoss()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "\n",
    "for t in range(50):\n",
    "    train_loss = train(train_dataloader, model, criterion, optimizer)\n",
    "    \n",
    "    valid_loss = validation(valid_dataloader, model, criterion)\n",
    "    \n",
    "    print('train loss : {}, validation loss : {}'.format(train_loss, valid_loss))\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
